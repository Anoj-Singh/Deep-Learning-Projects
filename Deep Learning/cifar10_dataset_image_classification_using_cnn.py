# -*- coding: utf-8 -*-
"""CIFAR10 dataset image classification using CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lu4yMQ55Lk8wh6YEFC3sTrdUH8jwNB4w
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from keras import layers, models
import tensorflow as tf
from keras.datasets import cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train.shape, x_test.shape

x_train[0]

x_train = x_train / 255
x_test = x_test / 255

x_train[0]

y_train[:5]

y_train = y_train.reshape(-1, )
y_test =y_test.reshape(-1, )

y_train[:5]

for i in range(10):
  plt.axis('off')
  plt.imshow(x_train[i])
  plt.show()

model = models.Sequential([
                         
                         layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(32, 32, 3), padding='same'),
                         layers.BatchNormalization(),
                         layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same'),
                         layers.BatchNormalization(),
                         layers.MaxPool2D(pool_size=2, strides=2),
                         layers.Dropout(0.3),
                         

                         layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),
                         layers.BatchNormalization(),
                         layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),
                         layers.BatchNormalization(),
                         layers.MaxPool2D(pool_size=2, strides=2),
                         layers.Dropout(0.4),
                         

                         layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),
                         layers.BatchNormalization(),
                         layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),
                         layers.BatchNormalization(),
                         layers.MaxPool2D(pool_size=2, strides=2),
                         layers.Dropout(0.5),
                         
                         layers.Flatten(),

                         layers.Dense(units=512, activation='relu'),
                         layers.Dense(units=256, activation='relu'),
                         layers.Dense(units=10, activation='softmax'),

])

model.summary()

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])

from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

early = EarlyStopping(monitor='val_loss',
                      patience=4,
                      verbose=1,
                      restore_best_weights=True)
filepath='model.h5'
check = ModelCheckpoint(filepath,
                        monitor='val_loss',
                        verbose=1,
                        save_best_only=True)

reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                              patience=3,
                              factor=0.2,
                              min_delta=0.00001,
                              verbose=1)

callback = [early, check, reduce_lr]

bs = 128
training_sample = len(x_train)
validation_sample = len(x_test)

model_history = model.fit(x_train, y_train, 
                          batch_size= bs, 
                          validation_data=(x_test, y_test), 
                          epochs=100, 
                          callbacks=callback,
                          steps_per_epoch=training_sample // bs, 
                          validation_steps= validation_sample // bs
                          )

model.evaluate(x_test, y_test)

History = pd.DataFrame(model_history.history)
History.head()

History[['loss', 'val_loss']].plot()
plt.ylabel('loss')
plt.xlabel('No. Of epochs')
plt.show()

History[['acc', 'val_acc']].plot()
plt.ylabel('Accuracy')
plt.xlabel('No. Of epochs')
plt.show()

y_pred = model.predict(x_test)
y_pred  = [np.argmax(j) for j in y_pred]
y_pred[:5], y_test[:5]

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns

print(classification_report(y_test, y_pred))
print('Accuracy is : ', accuracy_score(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, lw=3, fmt='d')
plt.show()

